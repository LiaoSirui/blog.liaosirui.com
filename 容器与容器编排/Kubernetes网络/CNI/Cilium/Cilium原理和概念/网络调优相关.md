## 主机路由切换为基于 BPF

“如果内核支持该选项，它将自动启用”

Kernel >= 5.10 的情况

```bash
# kubectl -n kube-system exec ds/cilium -- cilium status | grep -i bpf
Routing:                 Network: Native   Host: BPF
```

Kernel < 5.10：Legacy

在无法使用 eBPF 主机路由 (Host-Routing) 的情况下，网络数据包仍需在主机命名空间中穿越常规网络堆栈，iptables  会增加大量成本。通过禁用所有 Pod 流量的连接跟踪 (connection tracking) 要求，从而绕过 iptables 连接跟踪器  (iptables connection tracker)，可将这种遍历成本降至最低。

安装时选择

```bash
installNoConntrackIptablesRules: true
```

## BIG TCP

IPv6 BIG TCP 允许网络协议栈准备更大的 GSO（发送）和 GRO（接收）数据包，以减少协议栈的遍历次数，从而提高性能和延迟。它可减少 CPU 负载，有助于实现更高的速度（即 100Gbit/s 及以上）。为了让这些数据包通过协议栈，BIG TCP 在 IPv6  头之后添加了一个临时的 “逐跳”（Hop-By-Hop）头，并在通过线路传输数据包之前将其剥离。BIG TCP 可在双协议栈设置中运行，IPv4 数据包将使用旧的下限（64k），IPv6 数据包将使用新的较大下限（192k）。请注意，Cilium 假定 GSO 和 GRO 的默认内核值为 64k，只有在必要时才会进行调整，也就是说，如果启用了 BIG TCP，而当前的 GSO/GRO 最大值小于 192k，那么 Cilium  会尝试增加这些值；如果禁用了 BIG TCP，而当前的最大值大于 64k，那么 Cilium 会尝试减少这些值。BIG TCP  不需要更改网络接口 MTU。

```bash
# BIG TCP is not supported in tunneling mode
enableIPv4BIGTCP: true
enableIPv6BIGTCP: true
```

验证安装是否使用 IPv6 BIG TCP 运行，请在任何一个 Cilium pod 中运行 `cilium status`，并查找报告 "IPv6 BIG TCP" 状态的行，其状态应为 enabled

```bash
# kubectl -n kube-system exec ds/cilium -- cilium status | grep -i big
IPv4 BIG TCP:            Enabled   [65536]
IPv6 BIG TCP:            Enabled   [65536]
```

## BBR 拥塞控制

Cilium 的带宽管理器提供的围绕 MQ/FQ 设置的基础架构还允许对 Pod 使用 TCP BBR 拥塞控制。当 Pod 被暴露在  Kubernetes 服务背后，面对来自互联网的外部客户端时，BBR 尤其适用。BBR  可为互联网流量提供更高的带宽和更低的延迟，例如，事实证明，BBR 的吞吐量可比目前最好的基于损耗的拥塞控制高出 2,700 倍，队列延迟可降低  25 倍。

为了让 BBR 在 Pod 上可靠运行，它需要 5.18 或更高版本的内核。

BBR 还需要 eBPF Host-Routing，以保留网络数据包的套接字关联，直到数据包进入主机命名空间物理设备上的 FQ 队列 discipline。

```yaml
bandwidthManager:
  enabled: true
  bbr: true
```

## 巨型帧

最大传输单位（MTU）会对配置的网络吞吐量产生重大影响。Cilium 将自动检测底层网络设备的 MTU。因此，如果系统配置为使用巨型帧，Cilium 将自动使用巨型帧。